---
title: 最近爬数据遇到的一些坑
layout: post
tags:
  - machine_learning
  - daily
---
![](/media/files/2015/05/03.jpg)

最近在做一个问答网站主题推荐的算法,需要爬些知乎的数据,大约爬了1000个问题,9000+用户,200000+答案,我是用的这个[民间API](https://github.com/lufo816/zhihu-python),是我基于[这里](https://github.com/egrcc/zhihu-python)修改的,加了一些功能,改了一些bug(可是还有很多bug TAT).第一次爬这么多数据,随便乱写点遇到的坑.

- 爬数据前一定要对数据源的结构非常了解,知道可能遇到那些异常数据,尤其是自己写API的情况下,就不说被各种匿名用户和被和谐的答案坑多少回了.
- **爬数据时不要对数据进行处理!**这应该是我犯的最大的错.我在爬的同时把分词,过滤stopword什么的都做了,今天才意识到这样做很不好.第一违反了[do one thing and do it well](http://en.wikipedia.org/wiki/Unix_philosophy)的原则,第二在爬的同时做运算减慢了爬数据的效率,最重要的是如果你当时做的预处理有问题,或者你想用其他的方法进行预处理,那就意味着你就要再爬一次原始数据,幸好机智的我同时也存了原始数据,切记.
- 爬大量数据时不要都爬完再写文件,我爬这些数据用了几十小时,如果爬完再写如文件的话中间有什么问题那就要重爬,所以没爬一些数据就把这些写入文件.
- 并行化,因为爬数据主要时间花在网络请求上,所以并行化能明显加快速度.
- 先爬一小部分数据做测试,通过再把所有的都爬下来.可能你辛苦爬完所有数据,一做实验发现这个数据集有种种问题,所以要再找别的数据集,希望我不会遇到这么坑爹的问题.

暂时想到怎么多,Over.



