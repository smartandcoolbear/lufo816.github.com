---
title: ASR6:语言模型
layout: post
tags:
  - speech_recognition
---

我们已经知道了怎么做[连续语音识别](http://lufo.me/2015/07/21/ASR5.html)，但是这种方法还有一个问题，比如已经知道上一个单词是'Rock'，那么下一个单词是'star'，的概率明显要大于下一个单词是'spa'的概率，而我们之前的方法中并没有考虑到这一点。下面就介绍一下怎么在语音识别中使用语言模型(Language model)。

要解决这一问题，我们只需要在做Viterbi search的时候在单词之间加入一个根据语言模型算出来的cost就可以了，那么这个cost怎么算那？

根据贝叶斯规则，`P(W1|W2)=P(W1W2)/P(W2)`，我们只要统计文本中'Rock star'，'Rock spa'和'Rock'出现的次数，就能计算出已知'Rock'，下一个单词是'star'或'spa'的概率。其实这就是一个简单的语言模型了，但是只考虑前一个单词不太科学，可以考虑更多的单词。

只考虑这个单词本身出现的概率的模型叫unigram，考虑它的前一个单词的叫bigram，考虑前两个单词叫trigram，当然还可以考虑更多，这样模型就会更加复杂。trigram概率计算方法和前面类似。现在还有一个问题，怎么处理那些从来没出现过的单词那？这就要用到加平滑的方法。

这里我们主要讲一下Jelinek-Mercer smoothing，其他诸如Absolute discounting，Good Turing discounting可以自己学习。这个方法公式如下：

![](/media/files/2015/07/08.jpg)

这个方法很巧妙的地方在于它是递归地计算概率的，其中lambda是参数，可以设置为固定的，也可以为不同的单词序列使用EM算法训练不同的lambda，这样就能给没有出现过的单词序列分配概率了。

