---
title: ASR4:单个单词识别
layout: post
tags:
  - speech_recognition
---

我们现在已经知道了怎么[提取特征](http://lufo.me/2015/06/18/ASR1.html)和计算[编辑距离](http://lufo.me/2015/07/11/ASR2.html)，其实已经可以做语音识别了，语音每一帧(20ms)有一个高维的特征，这个特征就可以看作字符串中的一个字符，语音本身就可以看作一个字符串，与计算编辑距离类似，我们可以计算语音之间的距离，编辑距离计算时cost只有1和0，而语音之间的距离可以是两个帧之间的欧式距离或余弦距离，编辑距离计算中每个点可以向右，上，右上三个方向延伸，但语音由于是随着时间一直前进的，所以只能一直向右延伸，如图所示：

![](/media/files/2015/07/04.jpg)

这其实就是DTW算法，到这里其实我们已经能做语音识别了，但是还有下面几个问题：

1. 计算距离为什么用欧式距离，有没有更好地方法？

2. 人们说话时有时候某个音节会拖长，有时候又不会，这样的话一个音节的帧数不固定，怎么解决这个问题？

3. 每帧都比较时间复杂度是不是太高？

为了解决这几个问题，要引入隐马尔可夫模型(HMM)和混合高斯模型(GMM)。我们认为语音不同音节之间是符合高斯分布的，而不同的人声音不同，尤其是男女之间差别很大，所以同一个音节不同人说也是不同的，所以认为语音的分布是符合混合高斯模型的。那么模板的一个节点就可以看作一个混合高斯分布，语音数据与这个混合高斯模型的拟合程度可以用马氏距离(Mahalanobis distance)来衡量，即将语音数据代入模型中，然后计算它的对数的负数，如图所示：

![](/media/files/2015/07/05.jpg)

如果看不懂这里请自行学习混合高斯模型，这样就解决了第一个问题，另外两个问题需要用隐马尔可夫模型解决。

隐马尔可夫模型是关于时序的概率模型，描述一个由隐藏的马尔可夫链生成的不可观测的状态序列，再由状态序列生成观测序列。它有两个假设：

1. 当前状态只与前一个状态有关，与其他状态和观测序列无关。

2. 当前观测只与当前状态有关。

我们之前是以帧为单位做匹配，现在以state为单位，把每个音节分为n个state，每个音节分为多少state合适呢？这个很难说，有的可能需要多一些，有的少一些，这里我们取每个音节5个state，怎么把这些帧分成这5个state呢？我们用了k-means的思想。

以0这个单词为例，我们训练集中有10份0的录音，首先将这些帧均匀分为5份，每份的均值代表1个state，然后以这5个state作为模板，用10份训练数据做Viterbi search，Viterbi search与DTW类似，不同在于cost，每个帧与每个state间的cost分为两部分，第一部分是马氏距离，第二部分是这样算得：比如这10个训练数据一共有50帧属于第一个state，其中40帧的下一帧还是state 1，10帧的下一帧是state 2，这样如果当前状态是state 1，它下一状态是state 1的cost是它与state 1的马氏距离加上`-log(40/50)`，它下一状态是state 2的cost是它与state 2的马氏距离加上`-log(10/50)`(也符合隐马尔可夫模型的第一个假设)，这样做完一次Viterbi search后更新了每个帧所属的state，继续做Viterbi search，如果某一次Viterbi search之后每帧所属的state不变，说明结果收敛，就得到了这些state，然后以这些state为模板与输入语音做Viterbi search，得到最后的cost，cost最小的模板就是对应的单词。

以上就是单个单词识别(isolated word recognition)的方法，下一次继续讲连续语音识别(continuous speech recognition)。


